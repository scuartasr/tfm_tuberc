{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52ace60",
   "metadata": {},
   "source": [
    "## Configuración y librerías\n",
    "Asegúrate de ejecutar las celdas de carga/preprocesamiento equivalentes a `b_08_ml` antes de estas secciones (variables como `T`, `mu`, `sigma`, `ages`, `years`, `mat`, `mat_log`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304ddd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scuartasr/Documents/Maestría/Tesis/tfm_tuberc/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUT_DIR = modelos/outputs/output\n"
     ]
    }
   ],
   "source": [
    "# Librerías básicas\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# Ajustes de visualización\n",
    "sns.set(context='notebook', style='whitegrid')\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "# Directorio de salida\n",
    "OUT_DIR = os.environ.get('OUT_DIR', 'modelos/outputs/output')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print('OUT_DIR =', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19de543",
   "metadata": {},
   "source": [
    "## Funciones utilitarias (métricas)\n",
    "Mismas métricas que en `b_08_ml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd1a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(a, b):\n",
    "    return float(np.mean((a - b) ** 2))\n",
    "\n",
    "def rmse(a, b):\n",
    "    return float(np.sqrt(mse(a, b)))\n",
    "\n",
    "def mae(a, b):\n",
    "    return float(np.mean(np.abs(a - b)))\n",
    "\n",
    "def mape(a, b):\n",
    "    eps = 1e-12\n",
    "    return float(np.mean(np.abs((a - b) / (a + eps))) * 100)\n",
    "\n",
    "def smape(a, b):\n",
    "    denom = (np.abs(a) + np.abs(b))\n",
    "    return float(np.mean(2.0 * np.abs(a - b) / np.where(denom == 0, 1.0, denom)) * 100)\n",
    "\n",
    "def wape(a, b):\n",
    "    return float(np.sum(np.abs(a - b)) / (np.sum(np.abs(a)) + 1e-12) * 100)\n",
    "\n",
    "def rmse_log(a, b):\n",
    "    return float(np.sqrt(np.mean((np.log1p(a) - np.log1p(b)) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460aec0f",
   "metadata": {},
   "source": [
    "## Construcción de entradas APC\n",
    "Reutiliza la misma lógica que en `b_08_ml` para construir `X_train_apc`, `y_train`, etc. Aquí se define una función de ayuda que espera variables ya presentes en el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f304fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_apc_inputs(mat_log, ages, years, T, mu, sigma):\n",
    "    # Ejemplo mínimo; ajusta según tu versión de b_08_ml.\n",
    "    # Construye canales: logm_norm; feature_period_norm; feature_cohort_norm; age_mid_norm; target_period_norm\n",
    "    N = (mat_log.shape[1] - T) * mat_log.shape[0]\n",
    "    C = 5\n",
    "    X = np.zeros((N, T, C), dtype=np.float32)\n",
    "    y = np.zeros((N,), dtype=np.float32)\n",
    "    meta = []\n",
    "    idx = 0\n",
    "    ages_mid = np.array([(a[0] + a[1]) / 2.0 for a in ages], dtype=float)\n",
    "    ages_mid_norm = (ages_mid - ages_mid.mean()) / (ages_mid.std() + 1e-12)\n",
    "    years_arr = np.array(years, dtype=float)\n",
    "    years_norm = (years_arr - years_arr.mean()) / (years_arr.std() + 1e-12)\n",
    "    for ai in range(mat_log.shape[0]):\n",
    "        for t0 in range(mat_log.shape[1] - T):\n",
    "            window = mat_log[ai, t0:t0+T]\n",
    "            target = mat_log[ai, t0+T]\n",
    "            # Canal 0: logm_norm\n",
    "            X[idx, :, 0] = (window - mu) / (sigma + 1e-12)\n",
    "            # Canal 1: feature_period_norm (años de las features)\n",
    "            X[idx, :, 1] = years_norm[t0:t0+T]\n",
    "            # Canal 2: feature_cohort_norm (cohortes aproximadas)\n",
    "            cohorts = years_arr[t0:t0+T] - ages_mid[ai]\n",
    "            cohorts_norm = (cohorts - cohorts.mean()) / (cohorts.std() + 1e-12)\n",
    "            X[idx, :, 2] = cohorts_norm\n",
    "            # Canal 3: age_mid_norm (constante por edad)\n",
    "            X[idx, :, 3] = ages_mid_norm[ai]\n",
    "            # Canal 4: target_period_norm (año del objetivo)\n",
    "            X[idx, :, 4] = years_norm[t0+T-1]  # último año en ventana\n",
    "            y[idx] = (target - mu) / (sigma + 1e-12)\n",
    "            meta.append((ai, years_arr[t0+T]))\n",
    "            idx += 1\n",
    "    return X, y, meta\n",
    "\n",
    "# Ejemplo de uso (descomenta cuando tengas las variables en el entorno):\n",
    "# X_train_apc, y_train, meta_train = build_apc_inputs(mat_log_train, ages, years_train, T, mu, sigma)\n",
    "# X_val_apc, y_val, meta_val = build_apc_inputs(mat_log_val, ages, years_val, T, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07213982",
   "metadata": {},
   "source": [
    "## Modelo RNN (sustituto de CNN)\n",
    "Se definen variantes con `SimpleRNN` y `LSTM`. Usa la que prefieras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b648973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_apc(T, C, rnn_type='simple', rnn_units=64, dense_units=64, learning_rate=1e-3, dropout=0.0):\n",
    "    inputs = layers.Input(shape=(T, C))\n",
    "    if rnn_type == 'simple':\n",
    "        x = layers.SimpleRNN(rnn_units, return_sequences=True)(inputs)\n",
    "    elif rnn_type == 'lstm':\n",
    "        x = layers.LSTM(rnn_units, return_sequences=True)(inputs)\n",
    "    elif rnn_type == 'gru':\n",
    "        x = layers.GRU(rnn_units, return_sequences=True)(inputs)\n",
    "    else:\n",
    "        raise ValueError('rnn_type debe ser simple|lstm|gru')\n",
    "    if dropout > 0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Ejemplo de instanciación (ajusta T, C):\n",
    "# model_apc = build_rnn_apc(T=T, C=X_train_apc.shape[2], rnn_type='lstm', rnn_units=64, dense_units=64, learning_rate=1e-3)\n",
    "# model_apc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e4cb3",
   "metadata": {},
   "source": [
    "## Entrenamiento (EarlyStopping)\n",
    "Usa la misma configuración que en `b_08_ml`, cambiando únicamente el constructor del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6548adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# hist = model_apc.fit(X_train_apc, y_train, validation_data=(X_val_apc, y_val),\n",
    "#                      epochs=200, batch_size=256, callbacks=[es], verbose=1)\n",
    "# pd.DataFrame(hist.history).to_csv(os.path.join(OUT_DIR, 'hist_rnn_apc.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5becad41",
   "metadata": {},
   "source": [
    "## SHAP por canal (APC)\n",
    "Igual que en `b_08_ml`: se aplanan entradas (T*C), se envuelven las predicciones y se agregan por canal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fd041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de SHAP (ajusta a tu entorno de datos):\n",
    "# def predict_flat(X_flat):\n",
    "#     N = X_flat.shape[0]\n",
    "#     X_seq = X_flat.reshape(N, T, X_train_apc.shape[2])\n",
    "#     return model_apc.predict(X_seq, verbose=0).ravel()\n",
    "# \n",
    "# X_bg = X_train_apc.reshape(X_train_apc.shape[0], -1)\n",
    "# explainer = shap.Explainer(predict_flat, X_bg, algorithm='permutation')\n",
    "# shap_vals = explainer(X_bg)\n",
    "# shap_per_channel = np.mean(np.abs(shap_vals.values.reshape(X_train_apc.shape[0], T, X_train_apc.shape[2])), axis=(0,1))\n",
    "# pd.DataFrame({'canal': ['logm','period','cohort','age_mid','target_period'], 'mean_abs_shap': shap_per_channel}).to_csv(os.path.join(OUT_DIR, 'shap_channel_importance_rnn_apc.csv'), index=False)\n",
    "# plt.figure(figsize=(6,3)); plt.bar(['logm','period','cohort','age_mid','target_period'], shap_per_channel); plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, 'shap_channel_importance_rnn_apc.png'), dpi=160); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bc34a",
   "metadata": {},
   "source": [
    "## Validación y métricas\n",
    "Replica la validación (TRAIN+VAL) y los heatmaps del período completo como en `b_08_ml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d97e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de impresión de métricas (rellena con tus arrays):\n",
    "# print(f\"Train | MSE: {mse(y_train_true, y_train_pred):.4e} | RMSE: {rmse(y_train_true, y_train_pred):.4e} | MAE: {mae(y_train_true, y_train_pred):.4e} | MAPE: {mape(y_train_true, y_train_pred):.2f}% | sMAPE: {smape(y_train_true, y_train_pred):.2f}% | WAPE: {wape(y_train_true, y_train_pred):.2f}% | RMSE_log: {rmse_log(y_train_true, y_train_pred):.4f}\")\n",
    "# print(f\"Val   | MSE: {mse(y_val_true, y_val_pred):.4e} | RMSE: {rmse(y_val_true, y_val_pred):.4e} | MAE: {mae(y_val_true, y_val_pred):.4e} | MAPE: {mape(y_val_true, y_val_pred):.2f}% | sMAPE: {smape(y_val_true, y_val_pred):.2f}% | WAPE: {wape(y_val_true, y_val_pred):.2f}% | RMSE_log: {rmse_log(y_val_true, y_val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63140932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJ_ROOT = /Users/scuartasr/Documents/Maestría/Tesis/tfm_tuberc\n",
      "OUT_DIR   = /Users/scuartasr/Documents/Maestría/Tesis/tfm_tuberc/modelos/outputs/output\n",
      "DATA_PATH = /Users/scuartasr/Documents/Maestría/Tesis/tfm_tuberc/data\n",
      "Columnas CSV: ['ano', 't', 'gr_et', 'poblacion', 'conteo_defunciones', 'tasa_x100k', 'tasa']\n",
      "Usando columnas -> edad: gr_et | año: ano | tasa: tasa\n",
      "mu, sigma = -10.299621379372875 1.6257535810357813\n"
     ]
    }
   ],
   "source": [
    "# Configuración de entorno y datos (idéntico a b_08_ml, pero independiente)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "sns.set(context='notebook', style='whitegrid')\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "# Detecta la raíz del proyecto (dos niveles arriba de modelos/notebooks)\n",
    "_cwd = os.getcwd()\n",
    "if _cwd.endswith(os.path.join('modelos', 'notebooks')):\n",
    "    PROJ_ROOT = os.path.dirname(os.path.dirname(_cwd))\n",
    "else:\n",
    "    PROJ_ROOT = _cwd\n",
    "DATA_PATH = os.path.join(PROJ_ROOT, 'data')\n",
    "NOTEBOOK_NAME = 'b_09_ml'\n",
    "OUT_DIR = os.path.join(PROJ_ROOT, 'modelos', 'outputs', 'output')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print('PROJ_ROOT =', PROJ_ROOT)\n",
    "print('OUT_DIR   =', OUT_DIR)\n",
    "print('DATA_PATH =', DATA_PATH)\n",
    "\n",
    "# Carga de matrices y metadatos necesarios (replicar lógica de b_08_ml)\n",
    "mat_path = os.path.join(DATA_PATH, 'processed', 'mortalidad', 'tasas_mortalidad_gret_per.csv')\n",
    "if not os.path.exists(mat_path):\n",
    "    raise FileNotFoundError(f'No se encontró el archivo esperado: {mat_path}')\n",
    "df_long = pd.read_csv(mat_path)\n",
    "print('Columnas CSV:', list(df_long.columns))\n",
    "\n",
    "# Detecta nombres de columnas según variantes conocidas\n",
    "col_age_candidates = ['grupo_etario', 'gr_et', 'grupo_et', 'edad_grupo', 'grupo']\n",
    "col_year_candidates = ['periodo', 'anio', 'ano', 'year']\n",
    "col_rate_candidates = ['tasa_mortalidad', 'tasa', 'rate']\n",
    "\n",
    "def detect_col(cands):\n",
    "    for c in cands:\n",
    "        if c in df_long.columns:\n",
    "            return c\n",
    "    raise KeyError(f'No se encontró ninguna de las columnas esperadas: {cands}')\n",
    "\n",
    "col_age = detect_col(col_age_candidates)\n",
    "col_year = detect_col(col_year_candidates)\n",
    "col_rate = detect_col(col_rate_candidates)\n",
    "print('Usando columnas -> edad:', col_age, '| año:', col_year, '| tasa:', col_rate)\n",
    "\n",
    "# Pivot a matriz edad x año de tasas por 100k\n",
    "mat_obs_100k = df_long.pivot(index=col_age, columns=col_year, values=col_rate)\n",
    "ages_sorted = mat_obs_100k.index.to_numpy()\n",
    "years = mat_obs_100k.columns.to_numpy()\n",
    "mat = mat_obs_100k.to_numpy(dtype=float)\n",
    "mat_log = np.log(np.clip(mat, 1e-12, None))\n",
    "\n",
    "# División TRAIN/VAL por corte de año (igual a b_08_ml)\n",
    "cut_year = 2018\n",
    "if cut_year not in years:\n",
    "    # si el año 2018 no existe, usa el último año anterior a 2019\n",
    "    years_sorted = np.sort(years)\n",
    "    cut_year = years_sorted[years_sorted <= 2018][-1]\n",
    "idx_cut = int(np.where(years == cut_year)[0][0])\n",
    "mat_train = mat_log[:, :idx_cut+1]\n",
    "mat_val = mat_log[:, idx_cut+1:]\n",
    "train_years = years[:idx_cut+1]\n",
    "val_years = years[idx_cut+1:]\n",
    "\n",
    "# Normalización global del target (mu, sigma del TRAIN)\n",
    "mu = float(np.mean(mat_train))\n",
    "sigma = float(np.std(mat_train) + 1e-12)\n",
    "print('mu, sigma =', mu, sigma)\n",
    "\n",
    "# Parámetros de ventana T (igual que b_08_ml)\n",
    "T = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5273628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción de entradas base y APC (idéntico en estructura a b_08_ml)\n",
    "def build_base_inputs(mat_log, years, T, mu, sigma):\n",
    "    N = (mat_log.shape[1] - T) * mat_log.shape[0]\n",
    "    X = np.zeros((N, T, 1), dtype=np.float32)\n",
    "    y = np.zeros((N,), dtype=np.float32)\n",
    "    meta_age, meta_year = [], []\n",
    "    idx = 0\n",
    "    for ai in range(mat_log.shape[0]):\n",
    "        for t0 in range(mat_log.shape[1] - T):\n",
    "            window = mat_log[ai, t0:t0+T]\n",
    "            target = mat_log[ai, t0+T]\n",
    "            X[idx, :, 0] = (window - mu) / (sigma + 1e-12)\n",
    "            y[idx] = (target - mu) / (sigma + 1e-12)\n",
    "            meta_age.append(ai); meta_year.append(int(years[t0+T]))\n",
    "            idx += 1\n",
    "    return X, y, np.array(meta_age), np.array(meta_year)\n",
    "\n",
    "def build_apc_inputs(mat_log, ages_idx_to_mid, years, T, mu, sigma):\n",
    "    N = (mat_log.shape[1] - T) * mat_log.shape[0]\n",
    "    C = 5\n",
    "    X = np.zeros((N, T, C), dtype=np.float32)\n",
    "    y = np.zeros((N,), dtype=np.float32)\n",
    "    meta_age, meta_year = [], []\n",
    "    years_arr = years.astype(float)\n",
    "    years_norm = (years_arr - years_arr.mean()) / (years_arr.std() + 1e-12)\n",
    "    ages_mid = np.array([ages_idx_to_mid[i] for i in range(mat_log.shape[0])], dtype=float)\n",
    "    ages_mid_norm = (ages_mid - ages_mid.mean()) / (ages_mid.std() + 1e-12)\n",
    "    idx = 0\n",
    "    for ai in range(mat_log.shape[0]):\n",
    "        for t0 in range(mat_log.shape[1] - T):\n",
    "            window = mat_log[ai, t0:t0+T]\n",
    "            target = mat_log[ai, t0+T]\n",
    "            X[idx, :, 0] = (window - mu) / (sigma + 1e-12)\n",
    "            X[idx, :, 1] = years_norm[t0:t0+T]\n",
    "            cohorts = years_arr[t0:t0+T] - ages_mid[ai]\n",
    "            cohorts_norm = (cohorts - cohorts.mean()) / (cohorts.std() + 1e-12)\n",
    "            X[idx, :, 2] = cohorts_norm\n",
    "            X[idx, :, 3] = ages_mid_norm[ai]\n",
    "            X[idx, :, 4] = years_norm[t0+T-1]\n",
    "            y[idx] = (target - mu) / (sigma + 1e-12)\n",
    "            meta_age.append(ai); meta_year.append(int(years[t0+T]))\n",
    "            idx += 1\n",
    "    return X, y, np.array(meta_age), np.array(meta_year)\n",
    "\n",
    "# Mapear índices de grupo etario a punto medio (asumiendo etiquetas tipo \"[x,y)\")\n",
    "def parse_age_mid(age_label):\n",
    "    # Ajusta si el formato difiere\n",
    "    if isinstance(age_label, str) and ',' in age_label:\n",
    "        parts = age_label.strip('[]()').split(',')\n",
    "        a0, a1 = float(parts[0]), float(parts[1])\n",
    "        return (a0 + a1) / 2.0\n",
    "    try:\n",
    "        return float(age_label)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "ages_idx_to_mid = {i: parse_age_mid(lbl) for i, lbl in enumerate(ages_sorted)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a805a90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Construcción de datasets TRAIN/VAL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m X_train_b, y_train_b, meta_age_b, meta_year_b \u001b[38;5;241m=\u001b[39m build_base_inputs(mat_train, train_years, T, mu, sigma)\n\u001b[0;32m----> 3\u001b[0m X_val_b,   y_val_b,   meta_age_vb, meta_year_vb \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_base_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mval_years\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m X_train_apc, y_train, meta_age_apc, meta_year_apc \u001b[38;5;241m=\u001b[39m build_apc_inputs(mat_train, ages_idx_to_mid, train_years, T, mu, sigma)\n\u001b[1;32m      6\u001b[0m X_val_apc,   y_val,   meta_age_vapc, meta_year_vapc \u001b[38;5;241m=\u001b[39m build_apc_inputs(mat_val,   ages_idx_to_mid, val_years,   T, mu, sigma)\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mbuild_base_inputs\u001b[0;34m(mat_log, years, T, mu, sigma)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_base_inputs\u001b[39m(mat_log, years, T, mu, sigma):\n\u001b[1;32m      3\u001b[0m     N \u001b[38;5;241m=\u001b[39m (mat_log\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m T) \u001b[38;5;241m*\u001b[39m mat_log\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m     meta_age, meta_year \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "# Construcción de datasets TRAIN/VAL\n",
    "X_train_b, y_train_b, meta_age_b, meta_year_b = build_base_inputs(mat_train, train_years, T, mu, sigma)\n",
    "X_val_b,   y_val_b,   meta_age_vb, meta_year_vb = build_base_inputs(mat_val,   val_years,   T, mu, sigma)\n",
    "\n",
    "X_train_apc, y_train, meta_age_apc, meta_year_apc = build_apc_inputs(mat_train, ages_idx_to_mid, train_years, T, mu, sigma)\n",
    "X_val_apc,   y_val,   meta_age_vapc, meta_year_vapc = build_apc_inputs(mat_val,   ages_idx_to_mid, val_years,   T, mu, sigma)\n",
    "print('X_train_apc:', X_train_apc.shape, 'y_train:', y_train.shape)\n",
    "print('X_val_apc:  ', X_val_apc.shape,   'y_val:  ', y_val.shape)\n",
    "C = X_train_apc.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fecba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo RNN (sustituye CNN de b_08_ml)\n",
    "def build_rnn_apc(T, C, rnn_type='lstm', rnn_units=64, dense_units=64, learning_rate=1e-3, dropout=0.0):\n",
    "    inputs = layers.Input(shape=(T, C))\n",
    "    if rnn_type == 'simple':\n",
    "        x = layers.SimpleRNN(rnn_units, return_sequences=True)(inputs)\n",
    "    elif rnn_type == 'lstm':\n",
    "        x = layers.LSTM(rnn_units, return_sequences=True)(inputs)\n",
    "    elif rnn_type == 'gru':\n",
    "        x = layers.GRU(rnn_units, return_sequences=True)(inputs)\n",
    "    else:\n",
    "        raise ValueError('rnn_type debe ser simple|lstm|gru')\n",
    "    if dropout > 0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "epochs = 200; batch_size = 256\n",
    "es_apc = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "model_apc = build_rnn_apc(T=T, C=C, rnn_type='lstm', rnn_units=64, dense_units=64, learning_rate=1e-3, dropout=0.1)\n",
    "model_apc.summary()\n",
    "hist_apc = model_apc.fit(X_train_apc, y_train, validation_data=(X_val_apc, y_val), epochs=epochs, batch_size=batch_size, callbacks=[es_apc], verbose=1)\n",
    "pd.DataFrame(hist_apc.history).to_csv(os.path.join(OUT_DIR, 'hist_rnn_apc.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5177d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones TRAIN y VAL, y métricas\n",
    "yhat_train_scaled_apc = model_apc.predict(X_train_apc, verbose=0).ravel()\n",
    "yhat_val_scaled_apc   = model_apc.predict(X_val_apc,   verbose=0).ravel()\n",
    "ytrue_train_scaled_apc = y_train.ravel()\n",
    "ytrue_val_scaled_apc   = y_val.ravel()\n",
    "\n",
    "def print_metrics_split(y_true, y_pred, split):\n",
    "    print(f\"{split:<5} | MSE: {mse(y_true, y_pred):.4e} | RMSE: {rmse(y_true, y_pred):.4e} | MAE: {mae(y_true, y_pred):.4e} | MAPE: {mape(y_true, y_pred):.2f}% | sMAPE: {smape(y_true, y_pred):.2f}% | WAPE: {wape(y_true, y_pred):.2f}% | RMSE_log: {rmse_log(y_true, y_pred):.4f}\")\n",
    "\n",
    "print_metrics_split(ytrue_train_scaled_apc, yhat_train_scaled_apc, 'Train')\n",
    "print_metrics_split(ytrue_val_scaled_apc,   yhat_val_scaled_apc,   'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6beafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP por canal para RNN-APC (idéntico patrón al de b_08_ml)\n",
    "channel_names = ['logm_norm','feature_period_norm','feature_cohort_norm','age_mid_norm','target_period_norm']\n",
    "X_train_flat = X_train_apc.reshape(X_train_apc.shape[0], -1)\n",
    "\n",
    "def predict_flat(X_flat):\n",
    "    N = X_flat.shape[0]\n",
    "    X_seq = X_flat.reshape(N, T, C)\n",
    "    return model_apc.predict(X_seq, verbose=0).ravel()\n",
    "\n",
    "background_flat = X_train_flat\n",
    "expl_apc = shap.Explainer(predict_flat, background_flat, algorithm='permutation')\n",
    "sv_apc = expl_apc(X_train_flat)\n",
    "shap_arr = sv_apc.values.reshape(X_train_apc.shape[0], T, C)\n",
    "mean_abs_by_channel = np.mean(np.abs(shap_arr), axis=(0,1))\n",
    "pd.DataFrame({'canal': channel_names, 'mean_abs_shap': mean_abs_by_channel}).to_csv(os.path.join(OUT_DIR, 'shap_channel_importance_rnn_apc.csv'), index=False)\n",
    "plt.figure(figsize=(7,3)); plt.bar(channel_names, mean_abs_by_channel); plt.xticks(rotation=30); plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, 'shap_channel_importance_rnn_apc.png'), dpi=160); plt.close()\n",
    "\n",
    "# Heatmap SHAP por tiempo y canal\n",
    "mean_abs_by_t = np.mean(np.abs(shap_arr), axis=0)  # (T, C)\n",
    "plt.figure(figsize=(8,4)); sns.heatmap(mean_abs_by_t.T, cmap='viridis', cbar=True, yticklabels=channel_names); plt.xlabel('Lag (t)'); plt.ylabel('Canal'); plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, 'shap_time_by_channel_rnn_apc.png'), dpi=160); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0300f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm APC (agregación por ejes Edad, Cohorte, Período)\n",
    "# Construye una Explanation condensada con 3 features: Edad, Cohorte, Período\n",
    "# Sumando contribuciones SHAP de canales correspondientes\n",
    "def build_apc_axes_explanation(shap_arr, channel_names, X_train_flat):\n",
    "    # Mapea canales a ejes:\n",
    "    # Edad: age_mid_norm\n",
    "    # Cohorte: feature_cohort_norm\n",
    "    # Período: feature_period_norm + target_period_norm\n",
    "    ch_idx = {name:i for i,name in enumerate(channel_names)}\n",
    "    age_ch = ch_idx['age_mid_norm']\n",
    "    cohort_ch = ch_idx['feature_cohort_norm']\n",
    "    period_chs = [ch_idx['feature_period_norm'], ch_idx['target_period_norm']]\n",
    "    # shap_arr shape: (N, T, C)\n",
    "    age_shap = np.sum(np.abs(shap_arr[:,:,age_ch]), axis=1)    # (N,)\n",
    "    cohort_shap = np.sum(np.abs(shap_arr[:,:,cohort_ch]), axis=1) # (N,)\n",
    "    period_shap = np.sum(np.abs(shap_arr[:,:,period_chs]), axis=(1,2)) # (N,)\n",
    "    values = np.stack([age_shap, cohort_shap, period_shap], axis=1) # (N,3)\n",
    "    # Construir Explanation artificial\n",
    "    feature_names = ['Grupo etario','Cohorte','Período']\n",
    "    expl = shap.Explanation(values=values,\n",
    "                             base_values=np.zeros(values.shape[0]),\n",
    "                             data=X_train_flat[:,:3],\n",
    "                             feature_names=feature_names)\n",
    "    return expl\n",
    "\n",
    "expl_apc_axes = build_apc_axes_explanation(shap_arr, channel_names, X_train_flat)\n",
    "plt.figure(figsize=(8,4))\n",
    "shap.plots.beeswarm(expl_apc_axes, max_display=3, color=None, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, 'shap_beeswarm_rnn_apc_axes.png'), dpi=160)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b23874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación recursiva y heatmap completo 1979–2023 (100k)\n",
    "# Reconstruye predicciones TRAIN (one-step) y VAL (recursiva) en escala original 100k\n",
    "def inv_scale(y_scaled, mu, sigma):\n",
    "    return y_scaled * sigma + mu\n",
    "# Reconstruir matrices predichas a escala log y luego exponenciar a 100k\n",
    "# TRAIN one-step\n",
    "N_train = mat_train.shape[0] * (mat_train.shape[1] - T)\n",
    "yhat_train_log = inv_scale(yhat_train_scaled_apc, mu, sigma)\n",
    "# VAL recursiva: construir por edad, avanzando año a año\n",
    "m_train_pred = np.full_like(mat_train, np.nan)\n",
    "m_val_pred = np.full_like(mat_val, np.nan)\n",
    "# Rellena TRAIN con últimos T para facilitar contextos\n",
    "for ai in range(mat_train.shape[0]):\n",
    "    for t0 in range(mat_train.shape[1] - T):\n",
    "        tgt_idx = t0 + T\n",
    "        m_train_pred[ai, tgt_idx] = yhat_train_log[ai*(mat_train.shape[1]-T) + t0]\n",
    "# VAL recursiva\n",
    "for ai in range(mat_val.shape[0]):\n",
    "    # contexto inicial: últimos T puntos del TRAIN para esa edad\n",
    "    context_log = mat_train[ai, -T:].copy()\n",
    "    for t1 in range(mat_val.shape[1]):\n",
    "        # features APC para la ventana actual\n",
    "        win_years = val_years[max(0, t1-T+1):t1+1]\n",
    "        # Construye secuencia (T,C) acorde a build_apc_inputs\n",
    "        x_seq = np.zeros((1, T, C), dtype=np.float32)\n",
    "        # Canal 0: logm_norm\n",
    "        x_seq[0, :, 0] = (context_log - mu) / (sigma + 1e-12)\n",
    "        # Canal 1: feature_period_norm\n",
    "        years_arr = val_years.astype(float)\n",
    "        years_norm = (years_arr - years_arr.mean()) / (years_arr.std() + 1e-12)\n",
    "        t_start = max(0, t1-T+1)\n",
    "        pad_len = T - (t1 - t_start + 1)\n",
    "        fp = np.concatenate([np.zeros(pad_len), years_norm[t_start:t1+1]])\n",
    "        x_seq[0, :, 1] = fp\n",
    "        # Canal 2: feature_cohort_norm\n",
    "        age_mid = ages_idx_to_mid[ai]\n",
    "        cohorts = years_arr[t_start:t1+1] - age_mid\n",
    "        if cohorts.size > 1:\n",
    "            cohorts_norm = (cohorts - cohorts.mean()) / (cohorts.std() + 1e-12)\n",
    "        else:\n",
    "            cohorts_norm = np.zeros_like(cohorts)\n",
    "        fc = np.concatenate([np.zeros(pad_len), cohorts_norm])\n",
    "        x_seq[0, :, 2] = fc\n",
    "        # Canal 3: age_mid_norm (constante)\n",
    "        ages_mid_all = np.array([ages_idx_to_mid[i] for i in range(mat_train.shape[0])], dtype=float)\n",
    "        age_mid_norm = (ages_mid_all - ages_mid_all.mean()) / (ages_mid_all.std() + 1e-12)\n",
    "        x_seq[0, :, 3] = age_mid_norm[ai]\n",
    "        # Canal 4: target_period_norm (año actual)\n",
    "        x_seq[0, :, 4] = years_norm[max(0, t1-1)]\n",
    "        yhat_scaled = model_apc.predict(x_seq, verbose=0).ravel()[0]\n",
    "        yhat_log = inv_scale(yhat_scaled, mu, sigma)\n",
    "        m_val_pred[ai, t1] = yhat_log\n",
    "        # Actualiza contexto\n",
    "        context_log = np.concatenate([context_log[1:], [yhat_log]])\n",
    "# Convertir a tasas 100k\n",
    "mat_train_pred_100k = np.exp(m_train_pred) * 1e5\n",
    "mat_val_pred_100k   = np.exp(m_val_pred)   * 1e5\n",
    "mat_obs_100k_train  = np.exp(mat_train)    * 1e5\n",
    "mat_obs_100k_val    = np.exp(mat_val)      * 1e5\n",
    "# Unir y graficar\n",
    "mat_pred_full_100k = np.concatenate([mat_train_pred_100k, mat_val_pred_100k], axis=1)\n",
    "mat_obs_full_100k  = np.concatenate([mat_obs_100k_train, mat_obs_100k_val], axis=1)\n",
    "tick_years = years\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "vmin = np.nanmin(mat_obs_full_100k); vmax = np.nanmax(mat_obs_full_100k)\n",
    "sns.heatmap(mat_obs_full_100k, ax=ax[0], cmap='magma', vmin=vmin, vmax=vmax, cbar=True)\n",
    "ax[0].set_title('Observados (100k)'); ax[0].set_xlabel('Año'); ax[0].set_ylabel('Grupo etario')\n",
    "sns.heatmap(mat_pred_full_100k, ax=ax[1], cmap='magma', vmin=vmin, vmax=vmax, cbar=True)\n",
    "ax[1].set_title('Predichos RNN-APC (100k)'); ax[1].set_xlabel('Año'); ax[1].set_ylabel('Grupo etario')\n",
    "plt.savefig(os.path.join(OUT_DIR, 'rnn_apc_matrices_obs_pred_1979_2023_100k.png'), dpi=160)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
